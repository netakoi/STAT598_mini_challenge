{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os, os.path\n",
    "import pathlib\n",
    "from pandas import *\n",
    "from pathlib import Path\n",
    "import face_recognition\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# train\n",
    "folder_path = ' '\n",
    "processed_folder_path = ' '\n",
    "test_path = ' '\n",
    "model_dir = ' '\n",
    "csv_data = read_csv(folder_path+\"train.csv\")\n",
    "\n",
    "#==================================\n",
    "\n",
    "# train_small\n",
    "# folder_path = \n",
    "# processed_folder_path = \n",
    "# test_path = \n",
    "# model_dir = \n",
    "# csv_data = read_csv(folder_path+\"train_small.csv\")\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "rescale = (img_height, img_width)\n",
    "\n",
    "# reading CSV file\n",
    "category = csv_data['Category'].tolist()\n",
    "file_name = csv_data['File Name'].tolist()\n",
    "\n",
    "def rescale_image(file_path, rescale): # rescale image to a specfic size \n",
    "    # Load the image\n",
    "    image = cv2.imread(file_path, -1)\n",
    "    \n",
    "    if image is None:\n",
    "        return None    \n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image= cv2.resize(image, rescale)\n",
    "    #image = image.astype('float32')\n",
    "    #image /= 255.0\n",
    "\n",
    "    return image\n",
    "\n",
    "def extracter_opencv(file_path, rescale): # extract face using OpenCV [X]\n",
    "    multi_indicator = 0\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Read the input image\n",
    "    img = cv2.imread(file_path)\n",
    "    if (img is None) or (np.shape(img)[0] == 0):\n",
    "        return None, multi_indicator\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.01, 1)\n",
    "    # Crop\n",
    "    if np.shape(faces)[0] > 1: # if more than one person detected\n",
    "        face = np.zeros((rescale[0],rescale[1],np.shape(faces)[0]))\n",
    "        for i in range(0,np.shape(faces)[0]):\n",
    "            x, y, width, height = faces[i]\n",
    "            x, y = abs(x), abs(y)\n",
    "            x2, y2 = x + width, y + height\n",
    "            face_temp = gray[y:y2, x:x2]\n",
    "            face_temp = cv2.resize(face_temp, rescale)\n",
    "            face[:,:,i] = face_temp\n",
    "        multi_indicator = 1\n",
    "\n",
    "    elif np.shape(faces)[0] == 0: # No person\n",
    "        face = None\n",
    "\n",
    "    else: # only one person detected\n",
    "        face = np.zeros((rescale[0],rescale[1],np.shape(faces)[0]))\n",
    "        x, y, width, height = faces[0]\n",
    "        x, y = abs(x), abs(y)\n",
    "        x2, y2 = x + width, y + height\n",
    "        face = gray[y:y2, x:x2]\n",
    "        face = cv2.resize(face, rescale)\n",
    "\n",
    "    return face, multi_indicator\n",
    "\n",
    "def extracter_mtcnn(file_path, rescale): # extract face using MTCNN [X]\n",
    "\n",
    "    multi_indicator = 0\n",
    "\n",
    "    # load image\n",
    "    image = cv2.imread(file_path,-1)\n",
    "    if (image is None) or (np.shape(image)[0] == 0):\n",
    "        return None, multi_indicator\n",
    "\n",
    "    face = MTCNN().detect_faces(image)\n",
    "\n",
    "    # No face\n",
    "    if not face:\n",
    "        print(\"No face detected\")\n",
    "        return None, multi_indicator\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    \n",
    "    # cv2.imshow('face', image)\n",
    "    # cv2.waitKey()\n",
    "    # cv2.destroyAllWindows()\n",
    "    gray = np.zeros((rescale[0],rescale[1],3,len(face)))\n",
    "    if len(face) > 1:\n",
    "        multi_indicator = 1\n",
    "\n",
    "    for i in range(0,len(face)):\n",
    "        # Extract the face bounding box\n",
    "        x, y, width, height = face[i]['box']\n",
    "        x, y = abs(x), abs(y)\n",
    "        x2, y2 = x + width, y + height\n",
    "\n",
    "        image_temp = cv2.resize(image[y:y2, x:x2,:], rescale)\n",
    "        gray[:,:,:,i] = image_temp\n",
    "        #gray[:,:,:,i] = cv2.cvtColor(image_temp, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # temp = cv2.cvtColor(image_temp, cv2.COLOR_BGR2RGB)\n",
    "        # print(np.shape(temp))\n",
    "        # Crop, resize, and normalize the face\n",
    "        #gray[:,:,:,i] = cv2.resize(image[y:y2, x:x2, :], rescale)\n",
    "\n",
    "    return gray, multi_indicator\n",
    "\n",
    "def extracter_fcrg(file_path, rescale): # extract face using face recognition \n",
    "\n",
    "    multi_indicator = 0\n",
    "\n",
    "    # load image\n",
    "    image = cv2.imread(file_path,-1)\n",
    "    if (image is None) or (np.shape(image)[0] == 0):\n",
    "        print(\"None image\")\n",
    "        return None, multi_indicator\n",
    "\n",
    "    input_img = face_recognition.load_image_file(file_path)\n",
    "    face = face_recognition.face_locations(input_img)\n",
    "\n",
    "    # No face\n",
    "    if not face:\n",
    "        print(\"No face detected\")\n",
    "        return None, multi_indicator\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    gray = np.zeros((rescale[0],rescale[1],3,len(face)))\n",
    "    if len(face) > 1:\n",
    "        multi_indicator = 1\n",
    "        return None, multi_indicator #*********************\n",
    "\n",
    "    for i in range(0,len(face)):\n",
    "        # Extract the face bounding box\n",
    "        # x, y, width, height = face[i] \n",
    "        # x, y = abs(x), abs(y)\n",
    "        # x2, y2 = x + width, y + height\n",
    "\n",
    "        # top, right, bottom, left (64, 201, 219, 46)\n",
    "        #left, right, top, bottom = face[i]\n",
    "        top, right, bottom, left = face[i]\n",
    "        # image_temp = cv2.resize(image[y:y2, x:x2,:], rescale)\n",
    "        image_temp = cv2.resize(image[top:bottom, left:right,:], rescale)\n",
    "        gray[:,:,:,i] = image_temp\n",
    "        # cv2.imshow('face', image_temp)\n",
    "        # cv2.waitKey()\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "    return gray, multi_indicator\n",
    "\n",
    "def extracter_fcrg_prd(file_path, rescale): # extract face using face recognition for prediction\n",
    "\n",
    "    multi_indicator = 0\n",
    "\n",
    "    # load image\n",
    "    image = cv2.imread(file_path,-1)\n",
    "    if (image is None) or (np.shape(image)[0] == 0):\n",
    "        print(\"None image\")\n",
    "        return None, multi_indicator\n",
    "\n",
    "    input_img = face_recognition.load_image_file(file_path)\n",
    "    face = face_recognition.face_locations(input_img)\n",
    "\n",
    "    # No face\n",
    "    if not face:\n",
    "        print(\"No face detected\")\n",
    "        return None, multi_indicator\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "    gray = np.zeros((rescale[0],rescale[1],3,len(face)))\n",
    "    if len(face) > 1:\n",
    "        multi_indicator = 1\n",
    "\n",
    "    for i in range(0,len(face)):\n",
    "        # Extract the face bounding box\n",
    "        # x, y, width, height = face[i] \n",
    "        # x, y = abs(x), abs(y)\n",
    "        # x2, y2 = x + width, y + height\n",
    "\n",
    "        # top, right, bottom, left (64, 201, 219, 46)\n",
    "        #left, right, top, bottom = face[i]\n",
    "        top, right, bottom, left = face[i]\n",
    "        # image_temp = cv2.resize(image[y:y2, x:x2,:], rescale)\n",
    "        image_temp = cv2.resize(image[top:bottom, left:right,:], rescale)\n",
    "        gray[:,:,:,i] = image_temp\n",
    "        # cv2.imshow('face', image_temp)\n",
    "        # cv2.waitKey()\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "    return gray, multi_indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Image preprocessing (MTCNN): https://medium.com/@saranshrajput/face-detection-using-mtcnn-f3948e5d1acb\n",
    "#--------------------------------------------\n",
    "\n",
    "# [X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Image preprocessing (OpenCV): https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81\n",
    "#--------------------------------------------\n",
    "\n",
    "# [X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Image preprocessing (face_recognition): https://github.com/ageitgey/face_recognition\n",
    "#--------------------------------------------\n",
    "\n",
    "for i in range(0,len(category)):\n",
    "    if i%5 == 0:\n",
    "        print('Preprocessing... '+str(i)+'/'+str(len(category)))\n",
    "\n",
    "    try:\n",
    "        # if folder not eixsts, create folder:\n",
    "        if not os.path.exists(processed_folder_path+str(category[i])):\n",
    "            os.makedirs(processed_folder_path+str(category[i]))\n",
    "            image_count = 1   \n",
    "        else:\n",
    "            # count number of files in the folder\n",
    "            image_count = len([name for name in os.listdir(processed_folder_path+str(category[i])+\"\\\\\")])+1\n",
    "\n",
    "        face2, multi_indicator = extracter_fcrg(folder_path+str(i)+'.jpg', rescale)\n",
    "        if (face2 is None):\n",
    "            pass\n",
    "            # face0 = rescale_image(folder_path+str(i)+'.jpg', rescale)\n",
    "            # if (face0 is None):\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     # print(np.shape(face0))\n",
    "            #     face0_image = PIL.Image.fromarray(face0,'RGB')\n",
    "            #     #face0_image = face0_image.convert(\"L\")\n",
    "            #     face0_image.save(processed_folder_path+str(category[i])+'\\\\'+str(image_count)+'.jpg', 'JPEG')\n",
    "            #     image_count += 1\n",
    "        elif multi_indicator == 1:\n",
    "            # for j in range(np.shape(face2)[3]):\n",
    "            #     temp = np.asarray(face2[:,:,:,j], dtype=np.float32)\n",
    "            #     cv2.imwrite(processed_folder_path+str(category[i])+'\\\\'+str(image_count)+'.jpg', temp)\n",
    "            #     image_count += 1\n",
    "            pass   \n",
    "        else:\n",
    "            temp = np.asarray(face2[:,:,:,0], dtype=np.float32)\n",
    "            cv2.imwrite(processed_folder_path+str(category[i])+'\\\\'+str(image_count)+'.jpg', temp)\n",
    "            image_count += 1\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Generate training & validation set: https://www.tensorflow.org/tutorials/images/classification\n",
    "#--------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  processed_folder_path,\n",
    "  validation_split=0.3,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  processed_folder_path,\n",
    "  validation_split=0.3,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "# train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#   for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# NN setting (GhostFaceNet V2): https://github.com/HamadYA/GhostFaceNets\n",
    "#                               https://www.tensorflow.org/tutorials/images/classification\n",
    "#--------------------------------------------\n",
    "\n",
    "# [X]\n",
    "\n",
    "# import losses, train, GhostFaceNets\n",
    "\n",
    "# num_classes = len(class_names)\n",
    "# checkpoint_path = model_dir+'cp-{epoch:04d}.weights.h5'\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# # Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                 #  monitor='val_accuracy',\n",
    "#                                                 #  mode='max',\n",
    "#                                                 #  save_best_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "\n",
    "# emb_shape = 512\n",
    "# basic_model = GhostFaceNets.buildin_models(\"ghostnetv2\", dropout=0.5, emb_shape=emb_shape, output_layer='GDC', bn_momentum=0.9, bn_epsilon=1e-5,num_classes=num_classes)\n",
    "# basic_model = GhostFaceNets.add_l2_regularizer_2_model(basic_model, weight_decay=5e-4, apply_to_batch_normal=True)\n",
    "# basic_model = GhostFaceNets.replace_ReLU_with_PReLU(basic_model)\n",
    "# # basic_model = tf.keras.models.load_model('ghostnetv2_160_imagenet.h5')\n",
    "\n",
    "# # sch = [\n",
    "# #      {\"loss\": losses.ArcfaceLoss(scale=32), \"epoch\": 1},\n",
    "# #      {\"loss\": losses.ArcfaceLoss(scale=64), \"epoch\": 50},\n",
    "# #  ]\n",
    "# # #basic_model.compile(optimizer, loss=losses.ArcfaceLoss(scale=32), metrics=['accuracy'])\n",
    "# # #basic_model.compile(optimizer, sch, metrics=['accuracy'])\n",
    "# # basic_model.compile(optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# # basic_model.summary()\n",
    "\n",
    "# normalization_layer = layers.Rescaling(1./255)\n",
    "# data_augmentation = keras.Sequential(\n",
    "#    [\n",
    "#      layers.RandomFlip(\"horizontal\",\n",
    "#                        input_shape=(img_height,\n",
    "#                                     img_width,\n",
    "#                                    3)),\n",
    "#      layers.RandomRotation(0.3),\n",
    "#      layers.RandomZoom(0.3),\n",
    "#    ]\n",
    "# )\n",
    "\n",
    "# model = Sequential([\n",
    "#     data_augmentation,\n",
    "#     #normalization_layer,\n",
    "#     basic_model,\n",
    "#     layers.Dense(emb_shape/2, activation='relu'),\n",
    "#     layers.Dense(num_classes)\n",
    "#   ])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=0.1, momentum=0.9)\n",
    "# model.compile(optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['SparseCategoricalAccuracy'])\n",
    "# # model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# NN setting (VGGnet): https://github.com/rcmalli/keras-vggface\n",
    "#                      https://www.tensorflow.org/tutorials/images/classification\n",
    "#--------------------------------------------\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface import utils\n",
    "\n",
    "checkpoint_path = model_dir+'VGG_cp-{epoch:04d}.weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                #  monitor='val_accuracy',\n",
    "                                                #  mode='max',\n",
    "                                                #  save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "VGG = VGGFace(model = 'resnet50', include_top = False, input_shape = (224, 224, 3), pooling = 'avg')\n",
    "VGG.trainable = False\n",
    "\n",
    "# normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "   [\n",
    "     layers.RandomTranslation(height_factor = 0.15, width_factor = 0.15, input_shape = (224, 224, 3)),\n",
    "     layers.RandomRotation(factor = 0.15),\n",
    "     layers.RandomFlip(mode = \"horizontal\"),\n",
    "     layers.RandomZoom(height_factor = 0.05, width_factor = 0.05),\n",
    "     layers.RandomContrast(factor = 0.15)\n",
    "   ]\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    #normalization_layer,\n",
    "    VGG,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation = 'relu'),\n",
    "    layers.Dropout(0.15), # 0.3?\n",
    "    layers.Dense(100, activation = 'softmax')\n",
    "  ])\n",
    "\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),  \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-4), # 1e-2\n",
    "    metrics = ['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# ! Temporal: load weight\n",
    "#--------------------------------------------\n",
    "\n",
    "# model = tf.keras.models.load_model(model_dir +'0331_temp_model.keras')\n",
    "# model = tf.keras.models.load_model(model_dir +'0401_temp_model.keras')\n",
    "# model = tf.keras.models.load_model(model_dir +'0402_temp_model_v3.keras')\n",
    "# model.load_weights(model_dir +'temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Training\n",
    "#--------------------------------------------\n",
    "\n",
    "epochs=30\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  batch_size=batch_size,\n",
    "  callbacks=[cp_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Save model\n",
    "#--------------------------------------------\n",
    "\n",
    "model.save(model_dir+'VGG_final.keras')\n",
    "# model.save(model_dir+'Ghost_final.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Performance check\n",
    "#--------------------------------------------\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Generate prediction\n",
    "#--------------------------------------------\n",
    "\n",
    "#model = tf.keras.models.load_model(model_dir +'epoch_7_transfer_model.keras')\n",
    "# model = tf.keras.models.load_model(model_dir +'0401_temp_model.keras')\n",
    "\n",
    "image_count = len([name for name in os.listdir(test_path)])\n",
    "image_names=os.listdir(test_path)\n",
    "#img = tf.keras.utils.load_img(test_path, target_size=(img_height, img_width))\n",
    "\n",
    "if image_count != 4977:\n",
    "    print(\"You are doing someting wrong!!!\")\n",
    "\n",
    "for i in range(0,image_count):\n",
    "\n",
    "    try:\n",
    "        # detect face from test image\n",
    "        #face_test, test_indicator = extracter_mtcnn(test_path+filename, rescale)\n",
    "        face_test, test_indicator = extracter_fcrg_prd(test_path+str(i)+'.jpg', rescale)\n",
    "\n",
    "        # No face detected, predict from original image\n",
    "        if (face_test is None):\n",
    "            img = tf.keras.preprocessing.image.load_img(test_path+str(i)+'.jpg', target_size=(img_height, img_width, 3))\n",
    "            x=np.expand_dims(img, axis=0)\n",
    "            predictions = model.predict(x)\n",
    "            predicted_class = class_names[np.argmax(predictions)]\n",
    "\n",
    "        # Multiple faces are detected\n",
    "        elif test_indicator == 1:\n",
    "            print('multi-face detected')\n",
    "            for j in range(np.shape(face_test)[3]):\n",
    "                x=np.expand_dims(face_test[:,:,:,j], axis=0)\n",
    "                predictions = model.predict(x)\n",
    "                if j == 0:\n",
    "                    multi_predictor = predictions[0,np.argmax(predictions)]\n",
    "                    predicted_class = class_names[np.argmax(predictions)]\n",
    "                else:\n",
    "                    temp_predictor = predictions[0,np.argmax(predictions)]\n",
    "                    if temp_predictor > multi_predictor:\n",
    "                        multi_predictor = temp_predictor\n",
    "                        predicted_class = class_names[np.argmax(predictions)]\n",
    "                        \n",
    "        # Only one face detected\n",
    "        else:\n",
    "            #face_test_image = PIL.Image.fromarray(face_test[:,:,0])\n",
    "            #x=np.expand_dims(face_test_image, axis=0)\n",
    "            x=np.expand_dims(face_test[:,:,:,0], axis=0)\n",
    "            predictions = model.predict(x)\n",
    "            predicted_class = class_names[np.argmax(predictions)]\n",
    "\n",
    "        if i == 0:\n",
    "            prediction_result = [['Id', 'Category']]\n",
    "            prediction_result += [[0, predicted_class]]\n",
    "        else:\n",
    "            prediction_result += [[int(i), predicted_class]]\n",
    "\n",
    "        if i%5==0:\n",
    "            print(str(i)+' / '+str(image_count)+' completed...')\n",
    "\n",
    "        if image_count != 4977:\n",
    "            print(\"You are doing someting wrong!!!\")\n",
    "\n",
    "    except:\n",
    "        print('ERROR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        prediction_result += [[int(i), predicted_class]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Write prediction as CSV\n",
    "#--------------------------------------------\n",
    "\n",
    "import csv\n",
    "\n",
    "print(prediction_result)\n",
    "\n",
    "f = open('prediction.csv', 'w', newline='')\n",
    "writer = csv.writer(f)\n",
    "writer.writerows(prediction_result)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
